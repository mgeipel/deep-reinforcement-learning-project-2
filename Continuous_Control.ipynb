{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import namedtuple, deque\n",
    "from ddpg_agent import Agent \n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment for Windows\n",
    "#env = UnityEnvironment(file_name='Reacher_Windows_x86_64_parallel/Reacher.exe')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment for Linux\n",
    "env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/Projects/deep-reinforcement-learning-project-2/venv/lib/python3.6/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/markus/Projects/deep-reinforcement-learning-project-2/ddpg_agent.py:118: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), GRAD_CLIP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 1.39\tScore: 1.96\n",
      "Episode 20\tAverage Score: 2.56\tScore: 3.54\n",
      "Episode 30\tAverage Score: 5.82\tScore: 7.25\n",
      "Episode 40\tAverage Score: 9.61\tScore: 10.26\n",
      "Episode 50\tAverage Score: 14.67\tScore: 16.50\n",
      "Episode 60\tAverage Score: 17.19\tScore: 19.40\n",
      "Episode 70\tAverage Score: 18.11\tScore: 17.10\n",
      "Episode 80\tAverage Score: 18.73\tScore: 18.27\n",
      "Episode 90\tAverage Score: 23.99\tScore: 27.58\n",
      "Episode 100\tAverage Score: 34.09\tScore: 33.17\n",
      "Episode 110\tAverage Score: 36.51\tScore: 35.93\n",
      "Episode 120\tAverage Score: 37.94\tScore: 38.80\n",
      "Episode 130\tAverage Score: 37.76\tScore: 36.23\n",
      "Episode 140\tAverage Score: 37.82\tScore: 38.06\n",
      "Episode 150\tAverage Score: 37.04\tScore: 37.00\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=10)\n",
    "\n",
    "def ddpg(n_episodes=150, max_t=1000):\n",
    "    scores_deque = deque(maxlen=10)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] \n",
    "        states = env_info.vector_observations\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)\n",
    "            actions = np.clip(actions, -1, 1)    \n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "\n",
    "            if np.all(dones):\n",
    "                break\n",
    "\n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            states = next_states\n",
    "            score += np.mean(rewards)\n",
    "\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        if i_episode % 10 == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor_e%s.pth' % i_episode)\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic_e%s.pth' % i_episode)\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'score': scores})\n",
    "data.to_csv('scores.csv', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test Model\n",
    "\n",
    "every checkpoint is loaded and evaluated for 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/Projects/deep-reinforcement-learning-project-2/venv/lib/python3.6/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Checkpoint 10\tAverage Score: 2.02\n",
      "Train Checkpoint 20\tAverage Score: 3.45\n",
      "Train Checkpoint 30\tAverage Score: 6.21\n",
      "Train Checkpoint 40\tAverage Score: 15.02\n",
      "Train Checkpoint 50\tAverage Score: 15.41\n",
      "Train Checkpoint 60\tAverage Score: 18.56\n",
      "Train Checkpoint 70\tAverage Score: 17.91\n",
      "Train Checkpoint 80\tAverage Score: 22.09\n",
      "Train Checkpoint 90\tAverage Score: 30.54\n",
      "Train Checkpoint 100\tAverage Score: 34.57\n",
      "Train Checkpoint 110\tAverage Score: 34.09\n",
      "Train Checkpoint 120\tAverage Score: 38.31\n",
      "Train Checkpoint 130\tAverage Score: 34.91\n",
      "Train Checkpoint 140\tAverage Score: 34.55\n",
      "Train Checkpoint 150\tAverage Score: 36.33\n"
     ]
    }
   ],
   "source": [
    "avg_scores = []\n",
    "\n",
    "for train_checkpoint in range(10,151,10):\n",
    "    agent = Agent(state_size=state_size, action_size=action_size, random_seed=10)\n",
    "    agent.actor_local.load_state_dict(torch.load('checkpoint_actor_e%s.pth' % train_checkpoint))\n",
    "    agent.critic_local.load_state_dict(torch.load('checkpoint_critic_e%s.pth'% train_checkpoint))\n",
    "    for i_episode in range(1, 101):\n",
    "        scores = []\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        score = 0\n",
    "        for t in range(2000):\n",
    "            actions = agent.act(states, False)\n",
    "            actions = np.clip(actions, -1, 1)    \n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            states = env_info.vector_observations              # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "\n",
    "            if np.all(dones):\n",
    "                break\n",
    "\n",
    "            score += np.mean(rewards)\n",
    "\n",
    "        scores.append(score)       # save most recent score\n",
    "        print('\\rEpisode {}\\t Score: {:.2f}'.format(i_episode, np.mean(scores)), end=\"\")\n",
    "    print('\\rTrain Checkpoint {}\\tAverage Score: {:.2f}'.format(train_checkpoint, np.mean(scores)))    \n",
    "    avg_scores.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_avg = pd.DataFrame({'avg score': avg_scores})\n",
    "data_avg.index = np.arange(10,151,10)\n",
    "data_avg.to_csv('avg_scores.csv', sep='\\t', index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
